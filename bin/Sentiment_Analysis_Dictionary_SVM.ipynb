{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33cb62b",
   "metadata": {},
   "source": [
    "# 情感分析代码（情感词典与SVM）\n",
    "#### 备注：需要将 directory 变量更改成存放数据集的文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e391d53",
   "metadata": {},
   "source": [
    "### 1. 情感词典法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff94a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08d8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入并构建情感词典\n",
    "directory = \"C:\\\\Users\\\\keehu\\\\Downloads\\\\chinese_sentiment_dictionary-master\\\\file\\\\情感词典\\\\知网\\\\\"\n",
    "nlp_dict_path = {\"pos_dict\" : directory + \"正面评价词语（中文）.txt\",\n",
    "                 \"neg_dict\" : directory + \"负面评价词语（中文）.txt\"      \n",
    "                }\n",
    "\n",
    "nlp_dict = {}\n",
    "\n",
    "for name, path in nlp_dict_path.items():\n",
    "    curr_set = set()\n",
    "    with open(path) as f:\n",
    "        count = f.readline().split(\"\\t\")[1]\n",
    "        f.readline()\n",
    "        while f:\n",
    "            line = f.readline().strip()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            curr_set.add(line)\n",
    "    nlp_dict[name] = curr_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ae474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入程度词词典\n",
    "level_dict_path = directory + \"程度级别词语（中文）.txt\"\n",
    "level_dict = dict()\n",
    "level = [3,2,1.5,0.5,0.5,3,-1]\n",
    "with open(level_dict_path) as f:\n",
    "    count = f.readline().split(\"\\t\")[1]\n",
    "    f.readline()\n",
    "    i = 0\n",
    "    while i <= 7:\n",
    "        line = f.readline().strip()\n",
    "        if len(line.split()) > 1:\n",
    "            continue\n",
    "        if line == \"\":\n",
    "            i += 1\n",
    "        else:\n",
    "            level_dict[line] = level[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0351c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入停用词词典\n",
    "stopwords = [line.strip() for line in open(directory + \"stoplist1.txt\",'r', encoding = \"utf-8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c92588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def split_sentence(sentence):\n",
    "    global stopwords\n",
    "    seg_list = jieba.lcut(sentence, cut_all=False)\n",
    "    outputstr = []\n",
    "    for word in seg_list:\n",
    "        if word not in stopwords:\n",
    "            outputstr.append(word)\n",
    "    return outputstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e1aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找程度词并加权\n",
    "def search_level(words, score, j, level_p, level_dict, detected):\n",
    "    if j > level_p:\n",
    "        while j < len(words) and words[j] in level_dict:\n",
    "            score *= level_dict[words[j]]\n",
    "            level_p = j\n",
    "            detected.append(words[j])\n",
    "            j += 1\n",
    "    return level_p, score\n",
    "\n",
    "# 计算情感分数\n",
    "def sentiment_score(sentence, dict_set, level_dict):\n",
    "    words = split_sentence(sentence)\n",
    "    words.reverse()\n",
    "    score_stack = []\n",
    "    level_p = -1\n",
    "    total = 0\n",
    "    detected = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in dict_set[\"pos_dict\"]:\n",
    "            detected.append(words[i])\n",
    "            score = 1\n",
    "            level_p, score = search_level(words, score, i - 1, level_p, level_dict, detected)\n",
    "            level_p, score = search_level(words, score, i + 1, level_p, level_dict, detected)\n",
    "            total += score\n",
    "            \n",
    "        elif words[i] in dict_set[\"neg_dict\"]:\n",
    "            detected.append(words[i])\n",
    "            score = -1\n",
    "            level_p, score = search_level(words, score, i - 1, level_p, level_dict, detected)\n",
    "            level_p, score = search_level(words, score, i + 1, level_p, level_dict, detected)\n",
    "            total += score\n",
    "        i += 1\n",
    "            \n",
    "    return (total, 1 if total > 0 else 0, detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "852aaa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据行数与列数: (62774, 3)\n"
     ]
    }
   ],
   "source": [
    "# 读入数据集\n",
    "df = pd.read_csv(directory + \"online_shopping_10_cats.csv\", encoding=\"utf-8\")\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None,'display.max_colwidth', None)\n",
    "print(\"数据行数与列数: \"+str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099985a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\keehu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.543 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将停用词加入模型\n",
    "jieba.analyse.set_stop_words(directory + \"stoplist.txt\")\n",
    "jieba.suggest_freq([\"无语\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1256a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？为什么在文化上有着深刻纽带关系的中国和日本却在经济发展上有着极大的差异？为什么英国的北美殖民地造就了经济强大的美国，而西班牙的北美殖民却造就了范后的墨西哥？……很有价值，但不包括【中国近代史专业】。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延续，没有战后的民发反思，没有～，就不会让日本成为一个经济强国．当然，美国人也给日本人带来了耻辱．对日中关系也造成了深远的影响．文中揭露了＂东京审判＂中很多鲜为人知的东西．让人惊醒．唉！中国人民对日本的了解是不是太少了．</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵的有两点，一是他的理科知识不错，虽不能媲及罗素，但与理科知识很差的作家相比，他的文章可读性要强；其二是他人格和文风的朴实，不造作，不买弄，让人喜欢。读他的作品，犹如听一个好友和你谈心，常常唤起心中的强烈的共鸣。他的作品90年后的更好些。衷心祝愿周国平健康快乐，为世人写出更多好作品。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat  label  \\\n",
       "0  书籍      1   \n",
       "1  书籍      1   \n",
       "2  书籍      1   \n",
       "3  书籍      1   \n",
       "4  书籍      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                             review  \n",
       "0  ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。  \n",
       "1                                                                    作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。  \n",
       "2                                                                                                               作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？为什么在文化上有着深刻纽带关系的中国和日本却在经济发展上有着极大的差异？为什么英国的北美殖民地造就了经济强大的美国，而西班牙的北美殖民却造就了范后的墨西哥？……很有价值，但不包括【中国近代史专业】。  \n",
       "3                                                                                                       作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延续，没有战后的民发反思，没有～，就不会让日本成为一个经济强国．当然，美国人也给日本人带来了耻辱．对日中关系也造成了深远的影响．文中揭露了＂东京审判＂中很多鲜为人知的东西．让人惊醒．唉！中国人民对日本的了解是不是太少了．  \n",
       "4                                                                       作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵的有两点，一是他的理科知识不错，虽不能媲及罗素，但与理科知识很差的作家相比，他的文章可读性要强；其二是他人格和文风的朴实，不造作，不买弄，让人喜欢。读他的作品，犹如听一个好友和你谈心，常常唤起心中的强烈的共鸣。他的作品90年后的更好些。衷心祝愿周国平健康快乐，为世人写出更多好作品。  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3f4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['书籍', '平板', '手机', '水果', '洗发水', '热水器', '蒙牛', '衣服', '计算机', '酒店'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察数据类别\n",
    "df[\"cat\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5918be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理与分词\n",
    "df[\"review\"] = df[\"review\"].astype(str)\n",
    "df[\"splitted\"] = [split_sentence(i) for i in df[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4861ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行情感得分的计算\n",
    "pair = [sentiment_score(i, nlp_dict, level_dict) for i in df[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4649fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = [i[0] for i in pair]\n",
    "df[\"sentiment\"] = [i[1] for i in pair]\n",
    "df[\"detected\"] = [i[2] for i in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35dbc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25831,  5246],\n",
       "       [ 4462, 27235]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出混淆矩阵\n",
    "confusion_matrix(df[\"label\"], df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d88d4a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84     31077\n",
      "           1       0.84      0.86      0.85     31697\n",
      "\n",
      "    accuracy                           0.85     62774\n",
      "   macro avg       0.85      0.85      0.85     62774\n",
      "weighted avg       0.85      0.85      0.85     62774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 输出预测精确率与召回率\n",
    "print(classification_report(df[\"label\"], df[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be405e2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "书籍 0.7592833030381719\n",
      "平板 0.8588\n",
      "手机 0.87343951786483\n",
      "水果 0.8566\n",
      "洗发水 0.8495\n",
      "热水器 0.8017391304347826\n",
      "蒙牛 0.7919331037875061\n",
      "衣服 0.8996\n",
      "计算机 0.8081162324649298\n",
      "酒店 0.8170999999999999\n"
     ]
    }
   ],
   "source": [
    "# 观察各个类别的预测精确率\n",
    "arr = []\n",
    "for i in df.cat.unique():\n",
    "    wrong = df[(df.label != df.sentiment) & (df.cat == i)]\n",
    "    total = df[(df.cat == i)].size\n",
    "    print(i, 1 - wrong.size / total)\n",
    "    arr.append(wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3fdf4e",
   "metadata": {},
   "source": [
    "### 2. SVM模型法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72469cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集分成训练集和测试集\n",
    "neg= df[df.label==0][\"splitted\"]\n",
    "pos= df[df.label==1][\"splitted\"]\n",
    "y=np.concatenate((np.ones(len(pos)),np.zeros(len(neg))))\n",
    "x_train,x_test,y_train,y_test=train_test_split(np.concatenate((pos, neg)),y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a36fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建句子向量\n",
    "def build_sentence_vector(text,size,w2v_model):\n",
    "    vec=np.zeros(size).reshape((1,size))\n",
    "    count=0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec+=w2v_model.wv[word].reshape((1,size))\n",
    "            count+=1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count!=0:\n",
    "        vec/=count\n",
    "    return vec\n",
    " \n",
    "# 计算词向量\n",
    "def get_train_vecs(x_train, x_test):\n",
    "    n_dim=300 \n",
    "    \n",
    "    w2v_model=Word2Vec(vector_size=n_dim, window=5, sg=0, hs=0, negative=5, min_count=10)\n",
    "    w2v_model.build_vocab(x_train) \n",
    "    w2v_model.train(x_train, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs) \n",
    " \n",
    "    train_vecs=np.concatenate([build_sentence_vector(z,n_dim,w2v_model) for z in x_train])\n",
    "    \n",
    "    w2v_model.train(x_test,total_examples=w2v_model.corpus_count,epochs=w2v_model.epochs)\n",
    "    test_vecs=np.concatenate([build_sentence_vector(z,n_dim,w2v_model) for z in x_test])\n",
    "    return train_vecs, test_vecs, w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102b2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# 构建SVM模型与预测\n",
    "train_vecs, test_vecs, w2v_model = get_train_vecs(x_train, x_test)\n",
    "clf = SVC(kernel='rbf',verbose=True)\n",
    "clf.fit(train_vecs,y_train)\n",
    "test_pred = clf.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce842bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8725,  541],\n",
       "       [1294, 8273]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出混淆矩阵\n",
    "confusion_matrix(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f773e613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.94      0.90      9266\n",
      "         1.0       0.94      0.86      0.90      9567\n",
      "\n",
      "    accuracy                           0.90     18833\n",
      "   macro avg       0.90      0.90      0.90     18833\n",
      "weighted avg       0.91      0.90      0.90     18833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集预测精确率与召回率\n",
    "print(classification_report(y_test, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
